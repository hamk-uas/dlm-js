# Parameter estimation (maximum likelihood): MATLAB DLM vs dlm-js

> **Note:** This analysis was generated by Claude Opus 4.6 (GitHub Copilot) and reviewed by GPT-5.3-Codex (GitHub Copilot) based on reading the MATLAB DLM source code ([`mjlaine/dlm`](https://github.com/mjlaine/dlm)) and the dlm-js `dlmMLE` implementation.

## Cross-reference to original MATLAB DLM docs/source

The claims in this document are cross-checked against both the official tutorial and the source repository:

- **Main docs index:** [mjlaine.github.io/dlm](https://mjlaine.github.io/dlm/)
- **Tutorial (toolbox functions + parameter estimation):** [dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)
- **Repository root:** [github.com/mjlaine/dlm](https://github.com/mjlaine/dlm)
- **Core source files:** [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m), [`dlmsmo.m`](https://github.com/mjlaine/dlm/blob/master/dlmsmo.m), [`dlmgensys.m`](https://github.com/mjlaine/dlm/blob/master/dlmgensys.m)

Quick mapping from this comparison to original MATLAB references:

| Claim in this file | MATLAB reference |
|---|---|
| Kalman likelihood objective and recursion form | Tutorial §1.5 and §1.9 ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmsmo.m`](https://github.com/mjlaine/dlm/blob/master/dlmsmo.m) (`out.lik`) |
| `fminsearch`-based MLE in MATLAB DLM | Tutorial §1.9.1 ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) (`dlm_dooptim`) |
| `winds` parameter tying for diag(W) | Tutorial §1.9 / toolbox function `dlmfit` docs (§2.2) ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) |
| Optional observation error scaling (`fitv`) | Toolbox function `dlmfit` docs (§2.2) and source in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) |
| MCMC mode and posterior chain features | Tutorial §1.9 and ozone example (§3) ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) (`dlm_domcmc`) |
| AR coefficient estimation (`arphi`, `fitar`) | Tutorial §1.9.2 and §3 example ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); source support in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) and [`dlmgensys.m`](https://github.com/mjlaine/dlm/blob/master/dlmgensys.m) |
| Multivariate/missing-data support in MATLAB | Tutorial intro + demo 3 ([dlmdemo3](https://mjlaine.github.io/dlm/ex/dlmdemo3.html)); implementation in [`dlmsmo.m`](https://github.com/mjlaine/dlm/blob/master/dlmsmo.m) (`p>1`, NaN masking) |

**Scope note:** This comparison focuses on the univariate MLE workflow used by `dlm-js` `dlmMLE` (scalar observation, $p=1$), and does not attempt feature parity for MATLAB DLM multivariate/missing-data workflows.

## Objective function

Both optimize the same scalar likelihood form (for $p=1$ observations) — $-2 \log L$ from the Kalman filter prediction error decomposition:

$$-2 \log L = \sum_{t=1}^{n} \left[ \frac{v_t^2}{C_p^{(t)}} + \log C_p^{(t)} \right]$$

where $v_t = y_t - F x_{t|t-1}$ is the innovation and $C_p^{(t)} = F C_{t|t-1} F' + V^2$ is the innovation covariance. The `dlm_costfun` function (inside `dlmfit.m`) calls `dlmsmo(...,0,0)` (filter only, no smoother, no sample) and returns `out.lik`; we ran this under Octave. The dlm-js `makeKalmanLoss` in `src/mle.ts` computes the same per-step terms via `lax.scan` over the forward filter steps.

In practice, exact numeric equality is not expected because initialization and optimization procedures differ (e.g., `dlmfit` uses a two-pass prefit for initial state/covariance before optional optimization, as run under Octave).

## Parameterization

| Aspect | MATLAB DLM | dlm-js |
|--------|-----------|--------|
| Observation noise $s$ | Optionally fitted as a multiplicative factor $V \cdot e^{\theta_v}$ (controlled by `options.fitv`) | Always fitted: $s = e^{\theta_s}$ |
| State noise $w$ | $W_{ii} = (e^{\theta_{w,i}})^2$ | $W_{ii} = (e^{\theta_{w,i}})^2$ via `buildDiagW` |
| AR coefficients | Directly optimized (not log-transformed): $G(\text{arInds}) = \theta_\phi$ | Directly optimized (not log-transformed): $G(\text{arInds}) = \theta_\phi$ via `buildG` rank-1 update (AD-safe) |
| Parameter grouping | `options.winds` maps $\text{diag}(W)$ entries to shared parameters (e.g., `winds=[1,1,2,2]` ties states 1&2 and 3&4) | Each $W_{ii}$ is an independent parameter |

Both use the same positivity enforcement: log-space for variance parameters, then $e^{(\cdot)}$ to map back. The MATLAB version has an extra feature — `winds` — that lets you **tie** $\text{diag}(W)$ entries to shared parameters, reducing the optimization dimension when multiple states should share the same noise variance.

## Optimizer

| Aspect | MATLAB DLM | dlm-js |
|--------|-----------|--------|
| **Algorithm** | `fminsearch` (Nelder-Mead simplex) | Adam (gradient-based, 1st-order momentum) |
| **Gradient computation** | **None** — derivative-free | **Autodiff** via `valueAndGrad()` + reverse-mode AD through `lax.scan` |
| **Convergence** | Simplex shrinkage heuristic (no guaranteed rate for non-convex objectives) | Adaptive first-order method with bias-corrected moments; practical convergence depends on learning rate and objective conditioning |
| **Cost per optimizer step** | Multiple likelihood evaluations per simplex update (depends on dimension and simplex operations) | One `valueAndGrad` evaluation (forward + reverse AD through the loss) plus Adam state update |
| **Typical run budget** | 400 function evaluations (`options.maxfuneval` default) | 200 optimizer iterations (`maxIter` default) |
| **Compilation** | None (interpreted; tested under Octave, or optional `dlmmex` C MEX) | Optimization step is wrapped in a single `jit()`-traced function (forward filter + AD + Adam update) |
| **Jittability** | N/A | Fully jittable — optax Adam (as of v0.4.0, `count.item()` fix) |
| **Adam defaults** | N/A | `b1=0.9, b2=0.9, eps=1e-8` — b2=0.9 converges ~3× faster than canonical 0.999 on DLM likelihoods (measured across Nile, Kaisaniemi, ozone benchmarks) |
| **WASM performance** | N/A | ~<!-- timing:ckpt:nile:false-s -->1.8 s<!-- /timing --> for 60 iterations (Nile, n=100, m=2, b2=0.9, `checkpoint: false`); see [checkpointing note](#gradient-checkpointing-always-use-checkpoint-false) |

**Key tradeoff**: Nelder-Mead needs only function evaluations (no gradients), making it simple to apply and often robust on noisy/non-smooth surfaces. But cost grows quickly with parameter dimension because simplex updates require repeated objective evaluations. Adam with autodiff has higher per-step compute cost, but uses gradient information and often needs fewer optimization steps on smooth likelihoods like DLM filtering objectives.

### MLE vs MCMC: different objectives

Pure MLE minimises $-2 \log L$ without any prior on $W$. On real data such as satellite ozone measurements, this can produce degenerate solutions — e.g. most seasonal noise variances collapse to near-zero while one or two grow large — because the likelihood surface has a wide, nearly flat ridge. MATLAB MCMC uses a normal prior on $\log W$ entries that keeps them symmetric and away from zero, yielding a posterior mean at much higher $-2\log L$ but visually smoother, better-regularised results.

| Point | MATLAB MCMC | dlm-js MLE |
|-------|------------|------------|
| Ozone $-2\log L$ at MATLAB posterior W | 435.6 | — |
| Ozone $-2\log L$ at MLE optimum | — | 203.8 |
| Ozone trend shape | Smooth, symmetric seasonal noise | Same global trend, but seasonal W values degenerate |

If MCMC-like regularisation is needed, the recommended approach is MAP estimation: add a log-normal penalty on $W$ entries to the loss before differentiating. dlm-js `makeKalmanLoss` is a plain differentiable function and the penalty can be added outside of it before wrapping in `jit(valueAndGrad(...))`.

## Benchmark: same machine, same data

All timings measured on the same machine. The MATLAB DLM toolbox was run under Octave with `fminsearch` (Nelder-Mead, `maxfuneval=400` for Nile models, `maxfuneval=800` for Kaisaniemi). dlm-js uses `dlmMLE` (Adam + autodiff, `maxIter=300`, `b2=0.9` default, `checkpoint: false`, `wasm` backend). Octave timings are median of 5 runs after 1 warmup; dlm-js timings are single fresh-run wall-clock times (including first-call JIT overhead).

| Model | $n$ | $m$ | params | Octave `fminsearch` | dlm-js `dlmMLE` (wasm) | $-2\log L$ (Octave) | $-2\log L$ (dlm-js) |
|-------|---|---|--------|---------------------|------------------------|-----------------|-----------------|
| Nile, order=1, fit s+w | 100 | 2 | 3 | 2827 ms | <!-- timing:nile-mle:elapsed -->2585 ms<!-- /timing --> | 1104.6 | <!-- timing:mle-bench:nile-order1:lik -->1105.0<!-- /timing --> |
| Nile, order=1, fit w only | 100 | 2 | 2 | 1623 ms | — | 1104.7 | — |
| Nile, order=0, fit s+w | 100 | 1 | 2 | 610 ms | <!-- timing:mle-bench:nile-order0:elapsed -->1542 ms<!-- /timing --> | 1095.8 | <!-- timing:mle-bench:nile-order0:lik -->1095.8<!-- /timing --> |
| Kaisaniemi, trig, fit s+w | 117 | 4 | 5 | **failed** (NaN/Inf) | <!-- timing:mle-bench:kaisaniemi:elapsed -->3296 ms<!-- /timing --> | — | <!-- timing:mle-bench:kaisaniemi:lik -->341.4<!-- /timing --> |
| Energy, trig+AR, fit s+w+φ | 120 | 5 | 7 | — | <!-- timing:energy-mle:elapsed-ms -->6329 ms<!-- /timing --> | — | <!-- timing:energy-mle:lik -->443.1<!-- /timing --> |

Octave timings are from Octave with `fminsearch`; dlm-js timings are single fresh-run wall-clock times (including JIT overhead) from `pnpm run bench:mle`.

**Key observations:**
- **Nile (n=100, m=2):** Octave `fminsearch` is <!-- computed:static("octave-nile-order1-elapsed-ms") < slot("nile-mle:elapsed") ? "faster" : "slower" -->slower<!-- /computed --> on this dataset (see table). The Kalman filter is matrix multiplications in a loop, where Octave's LAPACK-backed vectorized ops are efficient per step. dlm-js pays one-time JIT compilation overhead; at n=100 these two effects roughly cancel.

- **Likelihood values:** Both converge to very similar $-2\log L$ values on Nile (difference ~<!-- computed:Math.abs(slot("mle-bench:nile-order1:lik") - static("octave-nile-order1-lik")).toFixed(1) -->0.4<!-- /computed -->), consistent with matching likelihood formulations under different optimization details.
<!-- ai-note: CLAIM: "b2=0.9 converges on Kaisaniemi" — lik is tracked by mle-bench:kaisaniemi:lik. Previously b2=0.9 gave 330.8 vs b2=0.999 gave 341.6; as of 2025-01 both are ~341. The "better optimum" claim has been removed. If :lik ever drops well below 341, that finding may return. -->
- **Kaisaniemi (m=4, 5 params):** The reported Octave `fminsearch` run (with `maxfuneval=800`) failed with NaN/Inf, while dlm-js converged in <!-- timing:mle-bench:kaisaniemi:iterations -->135<!-- /timing --> iterations (~<!-- timing:mle-bench:kaisaniemi:elapsed-s -->3.3 s<!-- /timing -->, b2=0.9), reaching $-2\log L =$ <!-- timing:mle-bench:kaisaniemi:lik -->341.4<!-- /timing -->. This is evidence in favor of gradient-based optimization on this case, but not a universal failure claim for Nelder-Mead.
- **Joint $s+w$ fitting:** dlm-js `dlmMLE` always fits both $s$ and $w$ together, while the MATLAB DLM toolbox (run under Octave) can fit $w$ only (`fitv=0`), which is faster when $s$ is known.

### Gradient checkpointing: always use `checkpoint: false`

`lax.scan` supports gradient checkpointing on the backward (AD) pass, controlled by a `checkpoint` option:

- **`true` (default):** √N segment checkpointing — stores only O(√N) intermediate carry values and re-runs the forward pass over each segment to reconstruct the rest during backprop. This trades ~2× computation for O(√N) memory.
- **`false`:** stores all N intermediate carry values — no recomputation on the backward pass.
- **number:** explicit segment size.

For typical DLM dataset sizes (n ≲ a few hundred), the carry at each time step is just an m-vector and an m×m matrix — negligible memory.

<!-- ai-note: CLAIM: "checkpoint: false is always faster" DEPENDS ON: ckpt:nile:speedup, ckpt:energy:speedup. CURRENT VALUES: +1% / -1% (negligible). The old claim "always faster" was written when speedup was +32%/+26%; it is now effectively zero on these benchmarks. See benchmark table below for current numbers. -->

**Benchmark (WASM, Float64, 60 iterations, 4 timed runs after 1 warmup):**

| Dataset | n | m | `checkpoint: false` | `checkpoint: true` (√N) | speedup |
|---------|---|---|--------------------|-----------------------|---------|
| Nile, order=1 | 100 | 2 | <!-- timing:ckpt:nile:false-ms -->1826 ms<!-- /timing --> | <!-- timing:ckpt:nile:true-ms -->1835 ms<!-- /timing --> | <!-- timing:ckpt:nile:speedup -->+0%<!-- /timing --> |
| Energy, order=1+trig1+ar1 | 120 | 5 | <!-- timing:ckpt:energy:false-ms -->2238 ms<!-- /timing --> | <!-- timing:ckpt:energy:true-ms -->2220 ms<!-- /timing --> | <!-- timing:ckpt:energy:speedup -->-1%<!-- /timing --> |

All explicit segment sizes (5, 11, 20, 40) performed similarly to `true`, confirming any overhead is purely from extra recomputation (not memory pressure).

**Conclusion:** For the current jax-js WASM backend and these dataset sizes (n ≤ 120), `checkpoint: false` and `checkpoint: true` have essentially identical performance (see <!-- timing:ckpt:nile:speedup -->+0%<!-- /timing --> / <!-- timing:ckpt:energy:speedup -->-1%<!-- /timing --> above). `dlmMLE` uses `checkpoint: false` as the default because it cannot be slower in theory (avoids all recomputation overhead) and poses no memory problem at these scales. For very long series (n ≫ 1000) where per-carry memory becomes a concern, an explicit segment size can be re-introduced at that time.


## MCMC (MATLAB DLM toolbox feature, not tested)

The MATLAB DLM toolbox has a second estimation mode (`options.mcmc=1`) that uses Adaptive Metropolis (AM) MCMC via Marko Laine's `mcmcrun` toolbox. We did not run the MCMC mode in our Octave benchmarks (it requires the separate `mcmcstat` dependency):

- Runs 5000 simulations (default `options.nsimu`)
- Normal prior on log-transformed parameters with configurable CVs (`options.varcv`, `options.vcv`)
- Returns full posterior chain, credible intervals, and point estimate (posterior mean of last half)
- Uses disturbance smoother for Gibbs-style state sampling

**dlm-js has no MCMC equivalent.** `dlmMLE` returns a point estimate only. Adding uncertainty quantification would require either:
- Porting `mcmcrun` (significant effort — it's a separate toolbox)
- Using the Hessian at the MLE optimum for approximate confidence intervals (cheap — just wrap `valueAndGrad` in another `grad`)
- Implementing stochastic gradient MCMC (e.g., SGLD) using the existing AD infrastructure

## Feature comparison summary

| Capability | MATLAB DLM | dlm-js `dlmMLE` |
|-----------|-----------|-----------------|
| MLE point estimate | ✅ `fminsearch` | ✅ Adam + autodiff |
| Gradient-based optimization | ❌ | ✅ |
| JIT compilation of optimizer | ❌ | ✅ |
| Fit observation noise `s` | ✅ (optional via `fitv`) | ✅ (always) |
| Fit state noise `w` | ✅ | ✅ |
| Fit AR coefficients `arphi` | ✅ | ✅ (`fitar: true`) |
| Tie W parameters (`winds`) | ✅ | ❌ (each W entry independent) |
| Custom cost function | ✅ (`options.fitfun`) | ❌ |
| MCMC posterior sampling | ✅ (Adaptive Metropolis via `mcmcrun`) | ❌ |
| State sampling for Gibbs | ✅ (disturbance smoother) | ❌ |
| Posterior uncertainty | ✅ (full chain) | ❌ (point estimate only) |
| Convergence diagnostics | ✅ (`chain`, `sschain` in MCMC mode) | ⚠️ Limited (`likHistory`, no posterior chain) |
| Runs in browser | ❌ | ✅ |
| MEX/WASM acceleration | ✅ (`dlmmex` optional) | ✅ (`wasm` backend; see [benchmark](#benchmark-same-machine-same-data)) |

## What dlm-js does differently

1. **Exact gradients** vs derivative-free simplex — for smooth likelihoods this often improves optimizer guidance, especially as parameter dimension grows (the Kaisaniemi benchmark is one example).
2. **JIT-wrapped optimization step** — forward filter + AD + parameter update are traced together in one optimization step function. JIT overhead currently dominates for small datasets (n=100); the advantage grows with larger n or more complex models.
3. **WASM backend** — runs in Node.js and the browser without native dependencies.
4. **Potentially more robust as dimension grows** — gradient-based optimization can remain practical in settings where derivative-free simplex methods become expensive or unstable.
5. **Joint AR coefficient estimation** — `fitar: true` jointly estimates observation noise, state variances, and AR coefficients in a single autodiff pass. The AR coefficients enter the G matrix via AD-safe rank-1 updates (`buildG`), keeping the entire optimization `jit()`-compilable.

## What MATLAB DLM does that dlm-js doesn't (yet)

1. **MCMC posterior sampling** — full Bayesian uncertainty quantification with priors.
2. **Parameter tying** (`winds`) — reduces optimization dimension for structured models.
3. **Custom fit functions** (`options.fitfun`) — user-supplied cost functions.
4. **V factor fitting** (`options.fitv`) — fits a multiplicative factor on V rather than V directly (useful when V is partially known from instrument specification).

## GPU + parallel Kalman filter via associative scan

Särkkä & García-Fernández [1] show that Bayesian filtering and smoothing recursions can be posed as **all-prefix-sums (parallel scan) operations**. For the linear-Gaussian case (i.e. Kalman filter + RTS smoother), each time step is an affine map and their composition is **associative**:

$$(A_2, b_2, \Sigma_2) \oplus (A_1, b_1, \Sigma_1) = (A_2 A_1,\; A_2 b_1 + b_2,\; A_2 \Sigma_1 A_2^\top + \Sigma_2)$$

This means the sequential `lax.scan` (O(n) depth) can be replaced by `lax.associativeScan` (O(log n) depth). The paper formulates parallel versions of both the forward filter and the RTS backward smoother, giving algebraically equivalent recursions in exact arithmetic while reducing parallel depth from linear to logarithmic [1, §3–4]. In floating-point arithmetic, operation reordering can still produce small numerical differences. Combined with a WebGPU backend, this gives two orthogonal dimensions of parallelism: across time steps (associative scan) and within each step's matrix operations (GPU ALUs).

**Production use in Pyro/NumPyro:** This is not just theoretical — Pyro's `GaussianHMM` distribution already uses parallel-scan Kalman filtering for inference, "allowing fast analysis of very long time series" [2]. Their `LinearHMM` (heavy-tailed variant) uses parallel auxiliary variable methods that reduce to `GaussianHMM`, then applies parallel-scan inference. The Pyro forecasting tutorial demonstrates this on BART ridership data (n = 78,888 hourly observations) where sequential filtering would be impractical [2]. NumPyro's HMM enumeration example uses the same approach via `scan()` with parallel semantics, explicitly citing Särkkä & García-Fernández for "reduc[ing] parallel complexity from O(length) to O(log(length))" [3].

**Impact on MLE**: Each `valueAndGrad(loss)` call currently runs a sequential Kalman filter under AD. With associative scan formulations, the theoretical sequential depth drops from O(n) to O(log n). However, as the crossover benchmark shows, this depth reduction is not realised in practice with jax-js because each compose operation dispatches as separate sequential GPU kernels. Until kernel fusion is available, MLE on WebGPU is strictly slower than WASM regardless of series length. For the energy demo (n=120, <!-- timing:energy-mle:iterations -->295<!-- /timing --> iters, ~<!-- timing:energy-mle:elapsed -->6.3 s<!-- /timing -->), the non-fused WebGPU path would be several hundred times slower per iteration.

### Implementation status

**The WebGPU associativeScan path is fully implemented and working.** `dlmFit` on the `webgpu` backend with `Float32` automatically uses this path (gated by `dtype === Float32 && device === 'webgpu'`):

- Solves the Discrete Algebraic Riccati Equation (DARE, MATLAB convention) for steady-state Kalman gain K_ss
- Reformulates the forward filter as an associative prefix scan per [1], with per-timestep affine elements composed via `lax.associativeScan`
- Uses Joseph form covariance update + symmetrization + diagonal clamping for Float32 stability
- Missing-data (NaN) support: observed timesteps use A_ss = G−K_ss·F; NaN timesteps use A = G (pure prediction, no Kalman gain update)

Requires the `fix/jit-scan-einsum-maxargs` branch of jax-js-nonconsuming (commit f09121d or later) which fixes JIT shape inference for einsum inside `lax.scan` / `lax.associativeScan` on WebGPU. See [upstream issue](../issues/jax-js-webgpu-jit-einsum.md) for full context.

### WebGPU vs WASM benchmark

`dlmFit` warm-run timings (jitted core, second of two runs):

| Model | $n$ | $m$ | wasm / f64 (scan) | webgpu / f32 (assocScan) |
|-------|-----|-----|-------------------|--------------------------|
| Nile, order=0 | 100 | 1 | <!-- timing:bb:nile-o0:wasm-f64 -->20 ms<!-- /timing --> | <!-- timing:bb:nile-o0:webgpu-f32 -->674 ms<!-- /timing --> |
| Nile, order=1 | 100 | 2 | <!-- timing:bb:nile-o1:wasm-f64 -->20 ms<!-- /timing --> | <!-- timing:bb:nile-o1:webgpu-f32 -->783 ms<!-- /timing --> |
| Kaisaniemi, trig | 117 | 4 | <!-- timing:bb:kaisaniemi:wasm-f64 -->22 ms<!-- /timing --> | <!-- timing:bb:kaisaniemi:webgpu-f32 -->843 ms<!-- /timing --> |
| Energy, trig+AR | 120 | 5 | <!-- timing:bb:trigar:wasm-f64 -->20 ms<!-- /timing --> | <!-- timing:bb:trigar:webgpu-f32 -->923 ms<!-- /timing --> |

**Why WebGPU is slower — and why there is no crossover with the current implementation:**

A crossover benchmark (Nile order=1, m=2) measured scaling at exponentially increasing N (WASM: 2 warmup + 4 timed runs median; WebGPU: same). At short series, WASM is dominated by fixed overhead (~20 ms — JIT result marshaling and JS↔WASM round-trips), so the per-step compute contribution is invisible:

| N | wasm/f64 | µs/step | webgpu/f32 | ratio |
|---|--------------|--------------|-----------------|-------|
| 100 | <!-- timing:scale:wasm-f64:n100 -->25 ms<!-- /timing --> | 245 | <!-- timing:scale:webgpu-f32:n100 -->868 ms<!-- /timing --> | 36× |
| 200 | <!-- timing:scale:wasm-f64:n200 -->21 ms<!-- /timing --> | 105 | <!-- timing:scale:webgpu-f32:n200 -->1213 ms<!-- /timing --> | 55× |
| 400 | <!-- timing:scale:wasm-f64:n400 -->21 ms<!-- /timing --> | 52 | <!-- timing:scale:webgpu-f32:n400 -->2220 ms<!-- /timing --> | 105× |
| 800 | <!-- timing:scale:wasm-f64:n800 -->22 ms<!-- /timing --> | 27 | <!-- timing:scale:webgpu-f32:n800 -->4209 ms<!-- /timing --> | 188× |
| 1600 | <!-- timing:scale:wasm-f64:n1600 -->21 ms<!-- /timing --> | 13 | <!-- timing:scale:webgpu-f32:n1600 -->7767 ms<!-- /timing --> | 343× |
| 3200 | <!-- timing:scale:wasm-f64:n3200 -->24 ms<!-- /timing --> | 7.5 | — | — |
| 6400 | <!-- timing:scale:wasm-f64:n6400 -->32 ms<!-- /timing --> | 5.0 | — | — |
| 12800 | <!-- timing:scale:wasm-f64:n12800 -->33 ms<!-- /timing --> | 2.6 | — | — |
| 25600 | <!-- timing:scale:wasm-f64:n25600 -->51 ms<!-- /timing --> | 2.0 | — | — |
| 51200 | <!-- timing:scale:wasm-f64:n51200 -->77 ms<!-- /timing --> | 1.5 | — | — |
| 102400 | <!-- timing:scale:wasm-f64:n102400 -->143 ms<!-- /timing --> | 1.4 | — | — |

Three findings:

1. **WASM stays flat up to N≈3200**, then grows roughly linearly. The per-step cost asymptotes around ~1.4 µs/step at N=102400 (~<!-- timing:scale:wasm-f64:n102400 -->143 ms<!-- /timing --> total). The flat region reflects fixed JIT/dispatch overhead, not compute.

2. **WebGPU scales as O(n)** — the ratio doubles with every doubling of N. The associativeScan O(log n) *depth* is never realised because jax-js dispatches each operation in the compose function (einsum $A_{comp}$, einsum $b_{comp}$, einsum $\Sigma_{comp}$, and the additions) as a separate sequential GPU kernel. The n−1 total compose calls therefore run serially, giving O(n × ops_per_compose) total GPU dispatches — more, not fewer, than the sequential O(n) filter. This is a kernel-fusion deficit: the associativeScan benefit can only be realised if all n/2 independent compose calls at each scan round are dispatched as a single batched kernel (one kernel per round × log n rounds). jax-js does not do this.

3. **No crossover exists**: extrapolating the WebGPU O(n) trend, its per-step dispatch cost is roughly 4–5 µs/step — about 3000× higher than WASM's asymptotic ~1.4 µs/step compute cost. WASM is faster at every N, and the gap grows with N.

**References:**
1. Särkkä, S. & García-Fernández, Á. F. (2020). [Temporal Parallelization of Bayesian Smoothers](https://arxiv.org/abs/1905.13002). *IEEE Transactions on Automatic Control*, 66(1), 299–306. — Poses Kalman filtering and RTS smoothing as associative all-prefix-sums operations; derives the parallel scan formulation for linear-Gaussian and general Bayesian models, reducing sequential depth from O(n) to O(log n).
2. Pyro contributors. [Forecasting II: state space models](https://pyro.ai/examples/forecasting_ii.html). — Demonstrates `GaussianHMM` with parallel-scan Kalman filtering on 78,888-step BART ridership data; also covers `LinearHMM` for heavy-tailed models with parallel auxiliary variable inference.
3. NumPyro contributors. [Example: Enumerate Hidden Markov Model](https://num.pyro.ai/en/latest/examples/hmm_enum.html). — Uses `scan()` with the parallel-scan algorithm of [1] to reduce parallel complexity of discrete HMM inference from O(length) to O(log(length)); demonstrates variable elimination combined with MCMC on polyphonic music data.
