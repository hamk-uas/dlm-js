# Parameter estimation (maximum likelihood): MATLAB DLM vs dlm-js

> **Note:** This analysis was generated by Claude Opus 4.6 (GitHub Copilot) and reviewed by GPT-5.3-Codex (GitHub Copilot) based on reading the MATLAB DLM source code ([`mjlaine/dlm`](https://github.com/mjlaine/dlm)) and the dlm-js `dlmMLE` implementation.

## Cross-reference to original MATLAB DLM docs/source

The claims in this document are cross-checked against both the official tutorial and the source repository:

- **Main docs index:** [mjlaine.github.io/dlm](https://mjlaine.github.io/dlm/)
- **Tutorial (toolbox functions + parameter estimation):** [dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)
- **Repository root:** [github.com/mjlaine/dlm](https://github.com/mjlaine/dlm)
- **Core source files:** [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m), [`dlmsmo.m`](https://github.com/mjlaine/dlm/blob/master/dlmsmo.m), [`dlmgensys.m`](https://github.com/mjlaine/dlm/blob/master/dlmgensys.m)

Quick mapping from this comparison to original MATLAB references:

| Claim in this file | MATLAB reference |
|---|---|
| Kalman likelihood objective and recursion form | Tutorial §1.5 and §1.9 ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmsmo.m`](https://github.com/mjlaine/dlm/blob/master/dlmsmo.m) (`out.lik`) |
| `fminsearch`-based MLE in MATLAB DLM | Tutorial §1.9.1 ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) (`dlm_dooptim`) |
| `winds` parameter tying for diag(W) | Tutorial §1.9 / toolbox function `dlmfit` docs (§2.2) ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) |
| Optional observation error scaling (`fitv`) | Toolbox function `dlmfit` docs (§2.2) and source in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) |
| MCMC mode and posterior chain features | Tutorial §1.9 and ozone example (§3) ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) (`dlm_domcmc`) |
| AR coefficient estimation (`arphi`, `fitar`) | Tutorial §1.9.2 and §3 example ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); source support in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) and [`dlmgensys.m`](https://github.com/mjlaine/dlm/blob/master/dlmgensys.m) |
| Multivariate/missing-data support in MATLAB | Tutorial intro + demo 3 ([dlmdemo3](https://mjlaine.github.io/dlm/ex/dlmdemo3.html)); implementation in [`dlmsmo.m`](https://github.com/mjlaine/dlm/blob/master/dlmsmo.m) (`p>1`, NaN masking) |

**Scope note:** This comparison focuses on the univariate MLE workflow used by `dlm-js` `dlmMLE` (scalar observation, $p=1$), and does not attempt feature parity for MATLAB DLM multivariate/missing-data workflows.

## Objective function

Both optimize the same scalar likelihood form (for $p=1$ observations) — $-2 \log L$ from the Kalman filter prediction error decomposition:

$$-2 \log L = \sum_{t=1}^{n} \left[ \frac{v_t^2}{C_p^{(t)}} + \log C_p^{(t)} \right]$$

where $v_t = y_t - F x_{t|t-1}$ is the innovation and $C_p^{(t)} = F C_{t|t-1} F' + V^2$ is the innovation covariance. The `dlm_costfun` function (inside `dlmfit.m`) calls `dlmsmo(...,0,0)` (filter only, no smoother, no sample) and returns `out.lik`; we ran this under Octave. The dlm-js `makeKalmanLoss` in `src/mle.ts` computes the same per-step terms via `lax.scan` over the forward filter steps.

In practice, exact numeric equality is not expected because initialization and optimization procedures differ (e.g., `dlmfit` uses a two-pass prefit for initial state/covariance before optional optimization, as run under Octave).

## Parameterization

| Aspect | MATLAB DLM | dlm-js |
|--------|-----------|--------|
| Observation noise $s$ | Optionally fitted as a multiplicative factor $V \cdot e^{\theta_v}$ (controlled by `options.fitv`) | Always fitted: $s = e^{\theta_s}$ |
| State noise $w$ | $W_{ii} = (e^{\theta_{w,i}})^2$ | $W_{ii} = (e^{\theta_{w,i}})^2$ via `buildDiagW` |
| AR coefficients | Directly optimized (not log-transformed): $G(\text{arInds}) = \theta_\phi$ | Directly optimized (not log-transformed): $G(\text{arInds}) = \theta_\phi$ via `buildG` rank-1 update (AD-safe) |
| Parameter grouping | `options.winds` maps $\text{diag}(W)$ entries to shared parameters (e.g., `winds=[1,1,2,2]` ties states 1&2 and 3&4) | Each $W_{ii}$ is an independent parameter |

Both use the same positivity enforcement: log-space for variance parameters, then $e^{(\cdot)}$ to map back. The MATLAB version has an extra feature — `winds` — that lets you **tie** $\text{diag}(W)$ entries to shared parameters, reducing the optimization dimension when multiple states should share the same noise variance.

## Optimizer

| Aspect | MATLAB DLM | dlm-js |
|--------|-----------|--------|
| **Algorithm** | `fminsearch` (Nelder-Mead simplex) | Adam (gradient-based, 1st-order momentum) |
| **Gradient computation** | **None** — derivative-free | **Autodiff** via `valueAndGrad()` + reverse-mode AD through `lax.scan` |
| **Convergence** | Simplex shrinkage heuristic (no guaranteed rate for non-convex objectives) | Adaptive first-order method with bias-corrected moments; practical convergence depends on learning rate and objective conditioning |
| **Cost per optimizer step** | Multiple likelihood evaluations per simplex update (depends on dimension and simplex operations) | One `valueAndGrad` evaluation (forward + reverse AD through the loss) plus Adam state update |
| **Typical run budget** | 400 function evaluations (`options.maxfuneval` default) | 200 optimizer iterations (`maxIter` default) |
| **Compilation** | None (interpreted; tested under Octave, or optional `dlmmex` C MEX) | Optimization step is wrapped in a single `jit()`-traced function (forward filter + AD + Adam update) |
| **Jittability** | N/A | Fully jittable — optax Adam (as of v0.4.0, `count.item()` fix) |
| **Adam defaults** | N/A | `b1=0.9, b2=0.9, eps=1e-8` — b2=0.9 converges ~3× faster than canonical 0.999 on DLM likelihoods (measured across Nile, Kaisaniemi, ozone benchmarks) |
| **WASM performance** | N/A | ~2 s for 300 iterations (Nile, n=100, m=2, b2=0.9); default `maxIter` is 200 |

**Key tradeoff**: Nelder-Mead needs only function evaluations (no gradients), making it simple to apply and often robust on noisy/non-smooth surfaces. But cost grows quickly with parameter dimension because simplex updates require repeated objective evaluations. Adam with autodiff has higher per-step compute cost, but uses gradient information and often needs fewer optimization steps on smooth likelihoods like DLM filtering objectives.

### MLE vs MCMC: different objectives

Pure MLE minimises $-2 \log L$ without any prior on $W$. On real data such as satellite ozone measurements, this can produce degenerate solutions — e.g. most seasonal noise variances collapse to near-zero while one or two grow large — because the likelihood surface has a wide, nearly flat ridge. MATLAB MCMC uses a normal prior on $\log W$ entries that keeps them symmetric and away from zero, yielding a posterior mean at much higher $-2\log L$ but visually smoother, better-regularised results.

| Point | MATLAB MCMC | dlm-js MLE |
|-------|------------|------------|
| Ozone $-2\log L$ at MATLAB posterior W | 435.6 | — |
| Ozone $-2\log L$ at MLE optimum | — | 203.8 |
| Ozone trend shape | Smooth, symmetric seasonal noise | Same global trend, but seasonal W values degenerate |

If MCMC-like regularisation is needed, the recommended approach is MAP estimation: add a log-normal penalty on $W$ entries to the loss before differentiating. dlm-js `makeKalmanLoss` is a plain differentiable function and the penalty can be added outside of it before wrapping in `jit(valueAndGrad(...))`.

## Benchmark: same machine, same data

All timings measured on the same machine. The MATLAB DLM toolbox was run under Octave with `fminsearch` (Nelder-Mead, `maxfuneval=400` for Nile models, `maxfuneval=800` for Kaisaniemi). dlm-js uses `dlmMLE` (Adam + autodiff, `maxIter=300`, `b2=0.9` default, `wasm` backend). Octave timings are median of 5 runs after 1 warmup; dlm-js timings are median of 3 runs after 1 warmup.

| Model | $n$ | $m$ | params | Octave `fminsearch` | dlm-js `dlmMLE` (wasm) | $-2\log L$ (Octave) | $-2\log L$ (dlm-js) |
|-------|---|---|--------|---------------------|------------------------|-----------------|-----------------|
| Nile, order=1, fit s+w | 100 | 2 | 3 | 2827 ms | 2730 ms | 1104.6 | 1105.0 |
| Nile, order=1, fit w only | 100 | 2 | 2 | 1623 ms | — | 1104.7 | — |
| Nile, order=0, fit s+w | 100 | 1 | 2 | 610 ms | 1970 ms | 1095.8 | 1095.8 |
| Kaisaniemi, trig, fit s+w | 117 | 4 | 5 | **failed** (NaN/Inf) | 3509 ms | — | 330.8 |
| Energy, trig+AR, fit s+w+φ | 120 | 5 | 7 | — | 6900 ms | — | 443.1 |

**Key observations:**
- **Nile (n=100, m=2):** Octave `fminsearch` is faster despite being an interpreted language — the Kalman filter is just matrix multiplications in a loop, where Octave's LAPACK-backed vectorized ops are efficient. dlm-js pays for JIT compilation overhead that doesn't amortize on a 100-observation dataset.
- **Likelihood values:** Both converge to very similar $-2\log L$ values on Nile (difference ~0.4), consistent with matching likelihood formulations under different optimization details.
- **Kaisaniemi (m=4, 5 params):** The reported Octave `fminsearch` run (with `maxfuneval=800`) failed with NaN/Inf, while dlm-js converged in 107 iterations (3.5 s, b2=0.9). This is evidence in favor of gradient-based optimization on this case, but not a universal failure claim for Nelder-Mead. Note: b2=0.9 also found a better optimum (−2logL=330.8) than b2=0.999 (341.6), suggesting the prior default was getting stuck in a plateau.
- **Joint $s+w$ fitting:** dlm-js `dlmMLE` always fits both $s$ and $w$ together, while the MATLAB DLM toolbox (run under Octave) can fit $w$ only (`fitv=0`), which is faster when $s$ is known.

## MCMC (MATLAB DLM toolbox feature, not tested)

The MATLAB DLM toolbox has a second estimation mode (`options.mcmc=1`) that uses Adaptive Metropolis (AM) MCMC via Marko Laine's `mcmcrun` toolbox. We did not run the MCMC mode in our Octave benchmarks (it requires the separate `mcmcstat` dependency):

- Runs 5000 simulations (default `options.nsimu`)
- Normal prior on log-transformed parameters with configurable CVs (`options.varcv`, `options.vcv`)
- Returns full posterior chain, credible intervals, and point estimate (posterior mean of last half)
- Uses disturbance smoother for Gibbs-style state sampling

**dlm-js has no MCMC equivalent.** `dlmMLE` returns a point estimate only. Adding uncertainty quantification would require either:
- Porting `mcmcrun` (significant effort — it's a separate toolbox)
- Using the Hessian at the MLE optimum for approximate confidence intervals (cheap — just wrap `valueAndGrad` in another `grad`)
- Implementing stochastic gradient MCMC (e.g., SGLD) using the existing AD infrastructure

## Feature comparison summary

| Capability | MATLAB DLM | dlm-js `dlmMLE` |
|-----------|-----------|-----------------|
| MLE point estimate | ✅ `fminsearch` | ✅ Adam + autodiff |
| Gradient-based optimization | ❌ | ✅ |
| JIT compilation of optimizer | ❌ | ✅ |
| Fit observation noise `s` | ✅ (optional via `fitv`) | ✅ (always) |
| Fit state noise `w` | ✅ | ✅ |
| Fit AR coefficients `arphi` | ✅ | ✅ (`fitar: true`) |
| Tie W parameters (`winds`) | ✅ | ❌ (each W entry independent) |
| Custom cost function | ✅ (`options.fitfun`) | ❌ |
| MCMC posterior sampling | ✅ (Adaptive Metropolis via `mcmcrun`) | ❌ |
| State sampling for Gibbs | ✅ (disturbance smoother) | ❌ |
| Posterior uncertainty | ✅ (full chain) | ❌ (point estimate only) |
| Convergence diagnostics | ✅ (`chain`, `sschain` in MCMC mode) | ⚠️ Limited (`likHistory`, no posterior chain) |
| Runs in browser | ❌ | ✅ |
| MEX/WASM acceleration | ✅ (`dlmmex` optional) | ✅ (`wasm` backend; see [benchmark](#benchmark-same-machine-same-data)) |

## What dlm-js does differently

1. **Exact gradients** vs derivative-free simplex — for smooth likelihoods this often improves optimizer guidance, especially as parameter dimension grows (the Kaisaniemi benchmark is one example).
2. **JIT-wrapped optimization step** — forward filter + AD + parameter update are traced together in one optimization step function. JIT overhead currently dominates for small datasets (n=100); the advantage grows with larger n or more complex models.
3. **WASM backend** — runs in Node.js and the browser without native dependencies.
4. **Potentially more robust as dimension grows** — gradient-based optimization can remain practical in settings where derivative-free simplex methods become expensive or unstable.
5. **Joint AR coefficient estimation** — `fitar: true` jointly estimates observation noise, state variances, and AR coefficients in a single autodiff pass. The AR coefficients enter the G matrix via AD-safe rank-1 updates (`buildG`), keeping the entire optimization `jit()`-compilable.

## What MATLAB DLM does that dlm-js doesn't (yet)

1. **MCMC posterior sampling** — full Bayesian uncertainty quantification with priors.
2. **Parameter tying** (`winds`) — reduces optimization dimension for structured models.
3. **Custom fit functions** (`options.fitfun`) — user-supplied cost functions.
4. **V factor fitting** (`options.fitv`) — fits a multiplicative factor on V rather than V directly (useful when V is partially known from instrument specification).

## Future: GPU + parallel Kalman filter via associative scan

Särkkä & García-Fernández [1] show that Bayesian filtering and smoothing recursions can be posed as **all-prefix-sums (parallel scan) operations**. For the linear-Gaussian case (i.e. Kalman filter + RTS smoother), each time step is an affine map and their composition is **associative**:

$$(A_2, b_2, \Sigma_2) \oplus (A_1, b_1, \Sigma_1) = (A_2 A_1,\; A_2 b_1 + b_2,\; A_2 \Sigma_1 A_2^\top + \Sigma_2)$$

This means the sequential `lax.scan` (O(n) depth) could be replaced by `lax.associativeScan` (O(log n) depth). The paper formulates parallel versions of both the forward filter and the RTS backward smoother, giving algebraically equivalent recursions in exact arithmetic while reducing parallel depth from linear to logarithmic [1, §3–4]. In floating-point arithmetic, operation reordering can still produce small numerical differences. Combined with a WebGPU backend, this would give two orthogonal dimensions of parallelism: across time steps (associative scan) and within each step's matrix operations (GPU ALUs).

**Production use in Pyro/NumPyro:** This is not just theoretical — Pyro's `GaussianHMM` distribution already uses parallel-scan Kalman filtering for inference, "allowing fast analysis of very long time series" [2]. Their `LinearHMM` (heavy-tailed variant) uses parallel auxiliary variable methods that reduce to `GaussianHMM`, then applies parallel-scan inference. The Pyro forecasting tutorial demonstrates this on BART ridership data (n = 78,888 hourly observations) where sequential filtering would be impractical [2]. NumPyro's HMM enumeration example uses the same approach via `scan()` with parallel semantics, explicitly citing Särkkä & García-Fernández for "reduc[ing] parallel complexity from O(length) to O(log(length))" [3].

**Impact on MLE**: Each `valueAndGrad(loss)` call currently runs a sequential Kalman filter under AD. With associative scan formulations, sequential depth can drop from O(n) to O(log n), which can accelerate each optimizer iteration on sufficiently long sequences and parallel hardware. For the energy demo (n=120, 300 iters, ~7.7 s), the theoretical serial depth reduction is from about 120 to about 7 per iteration.

**Practical caveats for dlm-js today:**
- **Short series ($n \approx 100$–$120$):** The depth reduction is real, but parallel scans add constant-factor overhead and GPU kernel dispatch for tiny $m=2$–$5$ matrices may dominate. Pyro's payoff comes from much longer series ($n \approx 79\text{k}$) [2].
- **WebGPU is float32-only:** The extra matrix multiplies in the tree reduction would amplify the existing float32 covariance instability (see numerical precision notes in `src/index.ts`).
- **`lax.associativeScan` is not yet available** in jax-js-nonconsuming.

The crossover point where GPU + associative scan clearly wins is likely around **$n > 500$, $m > 5$**, where GPU occupancy and parallel depth savings start to dominate dispatch overhead. This is the same algorithmic trick that makes S4/S5/Mamba efficient on GPU for long-sequence modeling.

**References:**
1. Särkkä, S. & García-Fernández, Á. F. (2020). [Temporal Parallelization of Bayesian Smoothers](https://arxiv.org/abs/1905.13002). *IEEE Transactions on Automatic Control*, 66(1), 299–306. — Poses Kalman filtering and RTS smoothing as associative all-prefix-sums operations; derives the parallel scan formulation for linear-Gaussian and general Bayesian models, reducing sequential depth from O(n) to O(log n).
2. Pyro contributors. [Forecasting II: state space models](https://pyro.ai/examples/forecasting_ii.html). — Demonstrates `GaussianHMM` with parallel-scan Kalman filtering on 78,888-step BART ridership data; also covers `LinearHMM` for heavy-tailed models with parallel auxiliary variable inference.
3. NumPyro contributors. [Example: Enumerate Hidden Markov Model](https://num.pyro.ai/en/latest/examples/hmm_enum.html). — Uses `scan()` with the parallel-scan algorithm of [1] to reduce parallel complexity of discrete HMM inference from O(length) to O(log(length)); demonstrates variable elimination combined with MCMC on polyphonic music data.
