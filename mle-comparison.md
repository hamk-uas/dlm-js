# Parameter estimation: MATLAB DLM vs dlm-js

> **Note:** This analysis was generated by Claude Opus 4.6 (GitHub Copilot) based on reading the MATLAB DLM source code ([`mjlaine/dlm`](https://github.com/mjlaine/dlm)) and the dlm-js `dlmMLE` implementation.

## Objective function (identical)

Both minimize the same quantity — $-2 \log L$ from the Kalman filter prediction error decomposition:

$$-2 \log L = \sum_{t=1}^{n} \left[ \frac{v_t^2}{C_p^{(t)}} + \log C_p^{(t)} \right]$$

where $v_t = y_t - F x_{t|t-1}$ is the innovation and $C_p^{(t)} = F C_{t|t-1} F' + V^2$ is the innovation covariance. The MATLAB cost function `dlm_costfun` (inside `dlmfit.m`) calls `dlmsmo(...,0,0)` (filter only, no smoother, no sample) and returns `out.lik`. The dlm-js `makeKalmanLoss` in `src/mle.ts` computes the same sum via `lax.scan` over the forward filter steps.

## Parameterization (same log-space transform)

| Aspect | MATLAB DLM | dlm-js |
|--------|-----------|--------|
| Observation noise `s` | Optionally fitted as a multiplicative factor `V·exp(θ_v)` (controlled by `options.fitv`) | Always fitted: `s = exp(θ_s)` |
| State noise `w` | `W(i,i) = exp(θ_{w,i})²` | `W(i,i) = exp(θ_{w,i})²` via `buildDiagW` |
| AR coefficients | Directly optimized (not log-transformed): `G(arinds) = θ_g` | Not yet optimized (AR phis are fixed) |
| Parameter grouping | `options.winds` maps W diag entries to shared parameters (e.g., `winds=[1,1,2,2]` ties states 1&2 and 3&4) | Each `W(i,i)` is an independent parameter |

Both use the same positivity enforcement: log-space for variance parameters, then `exp()` to map back. The MATLAB version has an extra feature — `winds` — that lets you **tie** W diagonal entries to shared parameters, reducing the optimization dimension when multiple states should share the same noise variance.

## Optimizer (fundamentally different)

| Aspect | MATLAB DLM | dlm-js |
|--------|-----------|--------|
| **Algorithm** | `fminsearch` (Nelder-Mead simplex) | Adam (gradient-based, 1st-order momentum) |
| **Gradient computation** | **None** — derivative-free | **Autodiff** via `valueAndGrad()` + reverse-mode AD through `lax.scan` |
| **Convergence** | Simplex shrinkage heuristic (no theoretical rate for non-convex) | Adaptive learning rate with bias-corrected moments; converges with $O(1/\sqrt{T})$ regret |
| **Per-iteration cost** | 1 forward filter pass (likelihood evaluation) | 1 forward + 1 backward pass (AD doubles the cost per iteration) |
| **Typical iterations** | 400 (`options.maxfuneval` default) | 200 (converges faster per iteration due to gradient info) |
| **Compilation** | None (interpreted MATLAB, or optional `dlmmex` C MEX) | Entire step is `jit()`-compiled: forward filter + AD + Adam update fused into one compiled kernel |
| **Jittability** | N/A | Fully jittable — pure array-op Adam (replaces optax which fails under `jit()` due to `count.item()`) |
| **WASM performance** | N/A | ~5 s for 300 iterations (Nile, n=100, m=2) |

**Key tradeoff**: Nelder-Mead needs only function evaluations (no gradients), making it trivial to implement and robust to non-smooth objectives. But it scales poorly with dimension — $O(d^2)$ simplex operations per iteration for $d$ parameters. Adam with autodiff has higher per-iteration cost (2× due to backward pass) but exploits gradient information, converging in far fewer iterations for smooth objectives like the DLM log-likelihood.

## MCMC (MATLAB-only feature)

The MATLAB DLM has a second estimation mode (`options.mcmc=1`) that uses Adaptive Metropolis (AM) MCMC via Marko Laine's `mcmcrun` toolbox:

- Runs 5000 simulations (default `options.nsimu`)
- Normal prior on log-transformed parameters with configurable CVs (`options.varcv`, `options.vcv`)
- Returns full posterior chain, credible intervals, and point estimate (posterior mean of last half)
- Uses disturbance smoother for Gibbs-style state sampling

**dlm-js has no MCMC equivalent.** `dlmMLE` returns a point estimate only. Adding uncertainty quantification would require either:
- Porting `mcmcrun` (significant effort — it's a separate toolbox)
- Using the Hessian at the MLE optimum for approximate confidence intervals (cheap — just wrap `valueAndGrad` in another `grad`)
- Implementing stochastic gradient MCMC (e.g., SGLD) using the existing AD infrastructure

## Feature comparison summary

| Capability | MATLAB DLM | dlm-js `dlmMLE` |
|-----------|-----------|-----------------|
| MLE point estimate | ✅ `fminsearch` | ✅ Adam + autodiff |
| Gradient-based optimization | ❌ | ✅ |
| JIT compilation of optimizer | ❌ | ✅ |
| Fit observation noise `s` | ✅ (optional via `fitv`) | ✅ (always) |
| Fit state noise `w` | ✅ | ✅ |
| Fit AR coefficients `arphi` | ✅ | ❌ (not yet) |
| Tie W parameters (`winds`) | ✅ | ❌ (each W entry independent) |
| Custom cost function | ✅ (`options.fitfun`) | ❌ |
| MCMC posterior sampling | ✅ (Adaptive Metropolis via `mcmcrun`) | ❌ |
| State sampling for Gibbs | ✅ (disturbance smoother) | ❌ |
| Posterior uncertainty | ✅ (full chain) | ❌ (point estimate only) |
| Convergence diagnostic | ✅ (`chain`, `sschain`) | ✅ (`likHistory`) |
| Runs in browser | ❌ | ✅ |
| MEX/WASM acceleration | ✅ (`dlmmex` optional) | ✅ (`wasm` backend, ~5 s for Nile) |

## What dlm-js does differently (and arguably better)

1. **Exact gradients** vs finite-difference-free simplex — Adam converges more reliably on smooth likelihoods.
2. **Full JIT compilation** — the entire optimization step (forward filter + AD + parameter update) compiles to a single fused kernel.
3. **WASM backend** — 29× faster than CPU interpreter, making MLE feasible for interactive use (~5 s for Nile).

## What MATLAB DLM does that dlm-js doesn't (yet)

1. **MCMC posterior sampling** — full Bayesian uncertainty quantification with priors.
2. **AR coefficient optimization** — `options.fitar` optimizes the autoregressive phis directly.
3. **Parameter tying** (`winds`) — reduces optimization dimension for structured models.
4. **Custom fit functions** (`options.fitfun`) — user-supplied cost functions.
5. **V factor fitting** (`options.fitv`) — fits a multiplicative factor on V rather than V directly (useful when V is partially known from instrument specification).
