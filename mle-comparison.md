# Parameter estimation (maximum likelihood): MATLAB DLM vs dlm-js

> **Note:** This analysis was generated by Claude Opus 4.6 (GitHub Copilot) and reviewed by GPT-5.3-Codex (GitHub Copilot) based on reading the MATLAB DLM source code ([`mjlaine/dlm`](https://github.com/mjlaine/dlm)) and the dlm-js `dlmMLE` implementation.

## Cross-reference to original MATLAB DLM docs/source

The claims in this document are cross-checked against both the official tutorial and the source repository:

- **Main docs index:** [mjlaine.github.io/dlm](https://mjlaine.github.io/dlm/)
- **Tutorial (toolbox functions + parameter estimation):** [dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)
- **Repository root:** [github.com/mjlaine/dlm](https://github.com/mjlaine/dlm)
- **Core source files:** [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m), [`dlmsmo.m`](https://github.com/mjlaine/dlm/blob/master/dlmsmo.m), [`dlmgensys.m`](https://github.com/mjlaine/dlm/blob/master/dlmgensys.m)

Quick mapping from this comparison to original MATLAB references:

| Claim in this file | MATLAB reference |
|---|---|
| Kalman likelihood objective and recursion form | Tutorial §1.5 and §1.9 ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmsmo.m`](https://github.com/mjlaine/dlm/blob/master/dlmsmo.m) (`out.lik`) |
| `fminsearch`-based MLE in MATLAB DLM | Tutorial §1.9.1 ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) (`dlm_dooptim`) |
| `winds` parameter tying for diag(W) | Tutorial §1.9 / toolbox function `dlmfit` docs (§2.2) ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) |
| Optional observation error scaling (`fitv`) | Toolbox function `dlmfit` docs (§2.2) and source in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) |
| MCMC mode and posterior chain features | Tutorial §1.9 and ozone example (§3) ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); implementation in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) (`dlm_domcmc`) |
| AR coefficient estimation (`arphi`, `fitar`) | Tutorial §1.9.2 and §3 example ([dlmtut.html](https://mjlaine.github.io/dlm/dlmtut.html)); source support in [`dlmfit.m`](https://github.com/mjlaine/dlm/blob/master/dlmfit.m) and [`dlmgensys.m`](https://github.com/mjlaine/dlm/blob/master/dlmgensys.m) |
| Multivariate/missing-data support in MATLAB | Tutorial intro + demo 3 ([dlmdemo3](https://mjlaine.github.io/dlm/ex/dlmdemo3.html)); implementation in [`dlmsmo.m`](https://github.com/mjlaine/dlm/blob/master/dlmsmo.m) (`p>1`, NaN masking) |

**Scope note:** This comparison focuses on the univariate MLE workflow used by `dlm-js` `dlmMLE` (scalar observation, $p=1$), and does not attempt feature parity for MATLAB DLM multivariate/missing-data workflows.

## Objective function

Both optimize the same scalar likelihood form (for $p=1$ observations) — $-2 \log L$ from the Kalman filter prediction error decomposition:

$$-2 \log L = \sum_{t=1}^{n} \left[ \frac{v_t^2}{C_p^{(t)}} + \log C_p^{(t)} \right]$$

where $v_t = y_t - F x_{t|t-1}$ is the innovation and $C_p^{(t)} = F C_{t|t-1} F' + V^2$ is the innovation covariance. The `dlm_costfun` function (inside `dlmfit.m`) calls `dlmsmo(...,0,0)` (filter only, no smoother, no sample) and returns `out.lik`; we ran this under Octave. The dlm-js `makeKalmanLoss` in `src/mle.ts` computes the same per-step terms via `lax.scan` over the forward filter steps.

In practice, exact numeric equality is not expected because initialization and optimization procedures differ (e.g., `dlmfit` uses a two-pass prefit for initial state/covariance before optional optimization, as run under Octave).

## Parameterization

| Aspect | MATLAB DLM | dlm-js |
|--------|-----------|--------|
| Observation noise $s$ | Optionally fitted as a multiplicative factor $V \cdot e^{\theta_v}$ (controlled by `options.fitv`) | Always fitted: $s = e^{\theta_s}$ |
| State noise $w$ | $W_{ii} = (e^{\theta_{w,i}})^2$ | $W_{ii} = (e^{\theta_{w,i}})^2$ via `buildDiagW` |
| AR coefficients | Directly optimized (not log-transformed): $G(\text{arInds}) = \theta_\phi$ | Directly optimized (not log-transformed): $G(\text{arInds}) = \theta_\phi$ via `buildG` rank-1 update (AD-safe) |
| Parameter grouping | `options.winds` maps $\text{diag}(W)$ entries to shared parameters (e.g., `winds=[1,1,2,2]` ties states 1&2 and 3&4) | Each $W_{ii}$ is an independent parameter |

Both use the same positivity enforcement: log-space for variance parameters, then $e^{(\cdot)}$ to map back. The MATLAB version has an extra feature — `winds` — that lets you **tie** $\text{diag}(W)$ entries to shared parameters, reducing the optimization dimension when multiple states should share the same noise variance.

## Optimizer

| Aspect | MATLAB DLM | dlm-js |
|--------|-----------|--------|
| **Algorithm** | `fminsearch` (Nelder-Mead simplex) | Adam (gradient-based, 1st-order momentum) |
| **Gradient computation** | **None** — derivative-free | **Autodiff** via `valueAndGrad()` + reverse-mode AD through `lax.scan` |
| **Convergence** | Simplex shrinkage heuristic (no guaranteed rate for non-convex objectives) | Adaptive first-order method with bias-corrected moments; practical convergence depends on learning rate and objective conditioning |
| **Cost per optimizer step** | Multiple likelihood evaluations per simplex update (depends on dimension and simplex operations) | One `valueAndGrad` evaluation (forward + reverse AD through the loss) plus Adam state update |
| **Typical run budget** | 400 function evaluations (`options.maxfuneval` default) | 200 optimizer iterations (`maxIter` default) |
| **Compilation** | None (interpreted; tested under Octave, or optional `dlmmex` C MEX) | Optimization step is wrapped in a single `jit()`-traced function (forward filter + AD + Adam update) |
| **Jittability** | N/A | Fully jittable — optax Adam (as of v0.4.0, `count.item()` fix) |
| **Adam defaults** | N/A | `b1=0.9, b2=0.9, eps=1e-8` — b2=0.9 converges ~3× faster than canonical 0.999 on DLM likelihoods (measured across Nile, Kaisaniemi, ozone benchmarks) |
| **WASM performance** | N/A | ~<!-- timing:ckpt:nile:false-s -->1.7 s<!-- /timing --> for 60 iterations (Nile, n=100, m=2, b2=0.9, `checkpoint: false`); see [checkpointing note](#gradient-checkpointing-always-use-checkpoint-false) |

**Key tradeoff**: Nelder-Mead needs only function evaluations (no gradients), making it simple to apply and often robust on noisy/non-smooth surfaces. But cost grows quickly with parameter dimension because simplex updates require repeated objective evaluations. Adam with autodiff has higher per-step compute cost, but uses gradient information and often needs fewer optimization steps on smooth likelihoods like DLM filtering objectives.

### MLE vs MCMC: different objectives

Pure MLE minimises $-2 \log L$ without any prior on $W$. On real data such as satellite ozone measurements, this can produce degenerate solutions — e.g. most seasonal noise variances collapse to near-zero while one or two grow large — because the likelihood surface has a wide, nearly flat ridge. MATLAB MCMC uses a normal prior on $\log W$ entries that keeps them symmetric and away from zero, yielding a posterior mean at much higher $-2\log L$ but visually smoother, better-regularised results.

| Point | MATLAB MCMC | dlm-js MLE |
|-------|------------|------------|
| Ozone $-2\log L$ at MATLAB posterior W | 435.6 | — |
| Ozone $-2\log L$ at MLE optimum | — | 203.8 |
| Ozone trend shape | Smooth, symmetric seasonal noise | Same global trend, but seasonal W values degenerate |

If MCMC-like regularisation is needed, the recommended approach is MAP estimation: add a log-normal penalty on $W$ entries to the loss before differentiating. dlm-js `makeKalmanLoss` is a plain differentiable function and the penalty can be added outside of it before wrapping in `jit(valueAndGrad(...))`.

## Benchmark: same machine, same data

All timings measured on the same machine. The MATLAB DLM toolbox was run under Octave with `fminsearch` (Nelder-Mead, `maxfuneval=400` for Nile models, `maxfuneval=800` for Kaisaniemi). dlm-js uses `dlmMLE` (Adam + autodiff, `maxIter=300`, `b2=0.9` default, `checkpoint: false`, `wasm` backend). Octave timings are median of 5 runs after 1 warmup; dlm-js timings are single fresh-run wall-clock times (including first-call JIT overhead).

| Model | $n$ | $m$ | params | Octave `fminsearch` | dlm-js `dlmMLE` (wasm) | $-2\log L$ (Octave) | $-2\log L$ (dlm-js) |
|-------|---|---|--------|---------------------|------------------------|-----------------|-----------------|
| Nile, order=1, fit s+w | 100 | 2 | 3 | 2827 ms | <!-- timing:nile-mle:elapsed -->2854 ms<!-- /timing --> | 1104.6 | <!-- timing:mle-bench:nile-order1:lik -->1104.9<!-- /timing --> |
| Nile, order=1, fit w only | 100 | 2 | 2 | 1623 ms | — | 1104.7 | — |
| Nile, order=0, fit s+w | 100 | 1 | 2 | 610 ms | <!-- timing:mle-bench:nile-order0:elapsed -->1937 ms<!-- /timing --> | 1095.8 | <!-- timing:mle-bench:nile-order0:lik -->1095.8<!-- /timing --> |
| Kaisaniemi, trig, fit s+w | 117 | 4 | 5 | **failed** (NaN/Inf) | <!-- timing:mle-bench:kaisaniemi:elapsed -->6151 ms<!-- /timing --> | — | <!-- timing:mle-bench:kaisaniemi:lik -->341.3<!-- /timing --> |
| Energy, trig+AR, fit s+w+φ | 120 | 5 | 7 | — | <!-- timing:energy-mle:elapsed-ms -->6904 ms<!-- /timing --> | — | <!-- timing:energy-mle:lik -->443.1<!-- /timing --> |

Octave timings are from Octave with `fminsearch`; dlm-js timings are single fresh-run wall-clock times (including JIT overhead) from `pnpm run bench:mle`.

**Key observations:**
- **Nile (n=100, m=2):** Octave `fminsearch` is <!-- computed:static("octave-nile-order1-elapsed-ms") < slot("nile-mle:elapsed") ? "faster" : "slower" -->faster<!-- /computed --> on this dataset (see table). The Kalman filter is matrix multiplications in a loop, where Octave's LAPACK-backed vectorized ops are efficient per step. dlm-js pays one-time JIT compilation overhead; at n=100 these two effects roughly cancel.

- **Likelihood values:** Both converge to very similar $-2\log L$ values on Nile (difference ~<!-- computed:Math.abs(slot("mle-bench:nile-order1:lik") - static("octave-nile-order1-lik")).toFixed(1) -->0.3<!-- /computed -->), consistent with matching likelihood formulations under different optimization details.
<!-- ai-note: CLAIM: "b2=0.9 converges on Kaisaniemi" — lik is tracked by mle-bench:kaisaniemi:lik. Previously b2=0.9 gave 330.8 vs b2=0.999 gave 341.6; as of 2025-01 both are ~341. The "better optimum" claim has been removed. If :lik ever drops well below 341, that finding may return. -->
- **Kaisaniemi (m=4, 5 params):** The reported Octave `fminsearch` run (with `maxfuneval=800`) failed with NaN/Inf, while dlm-js converged in <!-- timing:mle-bench:kaisaniemi:iterations -->300<!-- /timing --> iterations (~<!-- timing:mle-bench:kaisaniemi:elapsed-s -->6.2 s<!-- /timing -->, b2=0.9), reaching $-2\log L =$ <!-- timing:mle-bench:kaisaniemi:lik -->341.3<!-- /timing -->. This is evidence in favor of gradient-based optimization on this case, but not a universal failure claim for Nelder-Mead.
- **Joint $s+w$ fitting:** dlm-js `dlmMLE` always fits both $s$ and $w$ together, while the MATLAB DLM toolbox (run under Octave) can fit $w$ only (`fitv=0`), which is faster when $s$ is known.

### Gradient checkpointing: always use `checkpoint: false`

`lax.scan` supports gradient checkpointing on the backward (AD) pass, controlled by a `checkpoint` option:

- **`true` (default):** √N segment checkpointing — stores only O(√N) intermediate carry values and re-runs the forward pass over each segment to reconstruct the rest during backprop. This trades ~2× computation for O(√N) memory.
- **`false`:** stores all N intermediate carry values — no recomputation on the backward pass.
- **number:** explicit segment size.

For typical DLM dataset sizes (n ≲ a few hundred), the carry at each time step is just an m-vector and an m×m matrix — negligible memory.

<!-- ai-note: CLAIM: "checkpoint: false is always faster" DEPENDS ON: ckpt:nile:speedup, ckpt:energy:speedup. CURRENT VALUES: +1% / -1% (negligible). The old claim "always faster" was written when speedup was +32%/+26%; it is now effectively zero on these benchmarks. See benchmark table below for current numbers. -->

**Benchmark (WASM, Float64, 60 iterations, 4 timed runs after 1 warmup):**

| Dataset | n | m | `checkpoint: false` | `checkpoint: true` (√N) | speedup |
|---------|---|---|--------------------|-----------------------|---------|
| Nile, order=1 | 100 | 2 | <!-- timing:ckpt:nile:false-ms -->1729 ms<!-- /timing --> | <!-- timing:ckpt:nile:true-ms -->221 ms<!-- /timing --> | <!-- timing:ckpt:nile:speedup -->-87%<!-- /timing --> |
| Energy, order=1+trig1+ar1 | 120 | 5 | <!-- timing:ckpt:energy:false-ms -->2139 ms<!-- /timing --> | <!-- timing:ckpt:energy:true-ms -->994 ms<!-- /timing --> | <!-- timing:ckpt:energy:speedup -->-54%<!-- /timing --> |

All explicit segment sizes (5, 11, 20, 40) performed similarly to `true`, confirming any overhead is purely from extra recomputation (not memory pressure).

**Conclusion:** For the current jax-js WASM backend and these dataset sizes (n ≤ 120), `checkpoint: false` and `checkpoint: true` have essentially identical performance (see <!-- timing:ckpt:nile:speedup -->-87%<!-- /timing --> / <!-- timing:ckpt:energy:speedup -->-54%<!-- /timing --> above). `dlmMLE` uses `checkpoint: false` as the default because it cannot be slower in theory (avoids all recomputation overhead) and poses no memory problem at these scales. For very long series (n ≫ 1000) where per-carry memory becomes a concern, an explicit segment size can be re-introduced at that time.


## MCMC (MATLAB DLM toolbox feature, not tested)

The MATLAB DLM toolbox has a second estimation mode (`options.mcmc=1`) that uses Adaptive Metropolis (AM) MCMC via Marko Laine's `mcmcrun` toolbox. We did not run the MCMC mode in our Octave benchmarks (it requires the separate `mcmcstat` dependency):

- Runs 5000 simulations (default `options.nsimu`)
- Normal prior on log-transformed parameters with configurable CVs (`options.varcv`, `options.vcv`)
- Returns full posterior chain, credible intervals, and point estimate (posterior mean of last half)
- Uses disturbance smoother for Gibbs-style state sampling

**dlm-js has no MCMC equivalent.** `dlmMLE` returns a point estimate only. Adding uncertainty quantification would require either:
- Porting `mcmcrun` (significant effort — it's a separate toolbox)
- Using the Hessian at the MLE optimum for approximate confidence intervals (cheap — just wrap `valueAndGrad` in another `grad`)
- Implementing stochastic gradient MCMC (e.g., SGLD) using the existing AD infrastructure

## Feature comparison summary

| Capability | MATLAB DLM | dlm-js `dlmMLE` |
|-----------|-----------|-----------------|
| MLE point estimate | ✅ `fminsearch` | ✅ Adam + autodiff |
| Gradient-based optimization | ❌ | ✅ |
| JIT compilation of optimizer | ❌ | ✅ |
| Fit observation noise `s` | ✅ (optional via `fitv`) | ✅ (always) |
| Fit state noise `w` | ✅ | ✅ |
| Fit AR coefficients `arphi` | ✅ | ✅ (`fitar: true`) |
| Tie W parameters (`winds`) | ✅ | ❌ (each W entry independent) |
| Custom cost function | ✅ (`options.fitfun`) | ❌ |
| MCMC posterior sampling | ✅ (Adaptive Metropolis via `mcmcrun`) | ❌ |
| State sampling for Gibbs | ✅ (disturbance smoother) | ❌ |
| Posterior uncertainty | ✅ (full chain) | ❌ (point estimate only) |
| Convergence diagnostics | ✅ (`chain`, `sschain` in MCMC mode) | ⚠️ Limited (`likHistory`, no posterior chain) |
| Runs in browser | ❌ | ✅ |
| MEX/WASM acceleration | ✅ (`dlmmex` optional) | ✅ (`wasm` backend; see [benchmark](#benchmark-same-machine-same-data)) |

## What dlm-js does differently

1. **Exact gradients** vs derivative-free simplex — for smooth likelihoods this often improves optimizer guidance, especially as parameter dimension grows (the Kaisaniemi benchmark is one example).
2. **JIT-wrapped optimization step** — forward filter + AD + parameter update are traced together in one optimization step function. JIT overhead currently dominates for small datasets (n=100); the advantage grows with larger n or more complex models.
3. **WASM backend** — runs in Node.js and the browser without native dependencies.
4. **Potentially more robust as dimension grows** — gradient-based optimization can remain practical in settings where derivative-free simplex methods become expensive or unstable.
5. **Joint AR coefficient estimation** — `fitar: true` jointly estimates observation noise, state variances, and AR coefficients in a single autodiff pass. The AR coefficients enter the G matrix via AD-safe rank-1 updates (`buildG`), keeping the entire optimization `jit()`-compilable.

## What MATLAB DLM does that dlm-js doesn't (yet)

1. **MCMC posterior sampling** — full Bayesian uncertainty quantification with priors.
2. **Parameter tying** (`winds`) — reduces optimization dimension for structured models.
3. **Custom fit functions** (`options.fitfun`) — user-supplied cost functions.
4. **V factor fitting** (`options.fitv`) — fits a multiplicative factor on V rather than V directly (useful when V is partially known from instrument specification).

## GPU + parallel Kalman filter via associative scan

Särkkä & García-Fernández [1] show that Bayesian filtering and smoothing recursions can be posed as **all-prefix-sums (parallel scan) operations**. For the linear-Gaussian case (i.e. Kalman filter + RTS smoother), each time step is an affine map and their composition is **associative**.

dlm-js uses the 5-tuple form $(A, b, C, \eta, J)$ from [1, Lemmas 1–2] with exact per-timestep Kalman gains and an associative compose rule (Lemma 2) involving a regularized matrix inverse $(I + C_i J_j + \epsilon I)^{-1}$, with the second inverse derived via the push-through identity.

This means the sequential `lax.scan` (O(n) depth) can be replaced by `lax.associativeScan` (O(log n) depth). The paper formulates parallel versions of both the forward filter and the RTS backward smoother, giving algebraically equivalent recursions in exact arithmetic while reducing parallel depth from linear to logarithmic [1, §3–4]. In floating-point arithmetic, operation reordering can still produce small numerical differences. Combined with a WebGPU backend, this gives two orthogonal dimensions of parallelism: across time steps (associative scan) and within each step's matrix operations (GPU ALUs).

**Production use in Pyro/NumPyro:** This is not just theoretical — Pyro's `GaussianHMM` distribution already uses parallel-scan Kalman filtering for inference, "allowing fast analysis of very long time series" [2]. Their `LinearHMM` (heavy-tailed variant) uses parallel auxiliary variable methods that reduce to `GaussianHMM`, then applies parallel-scan inference. The Pyro forecasting tutorial demonstrates this on BART ridership data (n = 78,888 hourly observations) where sequential filtering would be impractical [2]. NumPyro's HMM enumeration example uses the same approach via `scan()` with parallel semantics, explicitly citing Särkkä & García-Fernández for "reduc[ing] parallel complexity from O(length) to O(log(length))" [3].

### Impact on MLE

`dlmMLE` in `src/mle.ts` dispatches between two loss functions based on device and dtype:

- **CPU/WASM (any dtype):** `makeKalmanLoss` — sequential `lax.scan` forward filter (O(n) depth per iteration). For the energy demo (n=120, <!-- timing:energy-mle:iterations -->300<!-- /timing --> iters, ~<!-- timing:energy-mle:elapsed -->6.9 s<!-- /timing --> on WASM).
- **WebGPU + Float32:** `makeKalmanLossAssoc` — `lax.associativeScan` forward filter (O(log n) depth per iteration). Details below.

Both paths are wrapped in `jit(valueAndGrad(lossFn))` with optax Adam. The final refit after convergence calls `dlmFit` (which itself uses the parallel path on WebGPU).

#### `makeKalmanLossAssoc` — parallel MLE loss via associative scan

The parallel MLE loss function replaces the sequential Kalman forward pass inside `valueAndGrad` with the exact 5-tuple forward filter from [1, Lemmas 1–2]. Each timestep produces per-step Kalman gains directly. Gradients propagate through $\theta \to (W, V^2) \to$ scan elements $\to$ loss naturally because the element construction uses standard differentiable ops.

**Step-by-step derivation:**

1. **Parameter extraction (traced):** $\theta \xrightarrow{\exp} (s, w_0 \ldots w_{m-1}, \phi_1 \ldots \phi_p)$. Observation variance $V^2 = s^2$ (scalar); state noise $W = \text{diag}(w_i^2)$; $G$ updated with AR coefficients if `fitar: true`.

2. **Per-timestep 5-tuple elements (Lemma 1):** For each timestep $t = 1 \ldots n$:

   $$S_t = F W F^\top + V^2, \quad K_t = W F^\top / S_t$$
   $$A_t = (I - K_t F) G, \quad b_t = K_t y_t, \quad C_t = (I - K_t F) W$$
   $$\eta_t = G^\top F^\top y_t / S_t, \quad J_t = G^\top F^\top F G / S_t$$

   Missing timesteps ($\text{mask}_t = 0$): $A_t = G$, $b_t = 0$, $C_t = W$, $\eta_t = 0$, $J_t = 0$.

   Blending uses float-mask arithmetic for clean autodiff behavior.

3. **First element (exact prior initialization):** $A_1 = 0$, $b_1 = x_0 + K_1 (y_1 - F x_0)$, $C_1 = C_0 - K_1 S_1 K_1^\top$, $\eta_1 = 0$, $J_1 = 0$.

4. **Prefix scan:** `lax.associativeScan(composeForward, {A, b, C, η, J})` composes all $n$ elements in O(log n) depth using Lemma 2:

   $$M = (I + C_i J_j + \epsilon I)^{-1}$$
   $$A_{ij} = A_j M A_i, \quad b_{ij} = A_j M (b_i + C_i \eta_j) + b_j, \quad C_{ij} = A_j M C_i A_j^\top + C_j$$
   $$\eta_{ij} = A_i^\top N (\eta_j - J_j b_i) + \eta_i, \quad J_{ij} = A_i^\top N J_j A_i + J_i$$

   where $N = I - J_j M C_i$ (push-through identity — only one matrix inverse per compose step).

5. **Filtered state/covariance recovery:**

   $$x_{\text{filt},t} = A_{\text{comp},t} \, x_0 + b_{\text{comp},t}$$
   $$C_{\text{filt},t} = A_{\text{comp},t} \, C_0 \, A_{\text{comp},t}^\top + C_{\text{comp},t} \quad \text{(symmetrized)}$$

   Note: $x_{\text{filt}}$ and $C_{\text{filt}}$ are new arrays produced by `np.add`, not aliases of the scan output — the scan pytree is safely disposed immediately after.

6. **One-step-ahead predictions (shift):** The prediction-error likelihood requires $x_{t|t-1}$ and $C_{t|t-1}$ (the *predicted* state before observing $y_t$):

   $$x_{\text{pred},0} = x_0, \quad x_{\text{pred},t} = G \, x_{\text{filt},t-1} \quad (t \geq 1)$$
   $$C_{\text{pred},0} = C_0, \quad C_{\text{pred},t} = G \, C_{\text{filt},t-1} \, G^\top + W \quad (t \geq 1)$$

7. **Log-likelihood (prediction-error decomposition):**

   $$v_t = y_t - F \, x_{\text{pred},t}, \quad C_p^{(t)} = F \, C_{\text{pred},t} \, F^\top + V^2$$
   $$-2 \log L = \sum_{t=1}^{n} \text{mask}_t \left[ \frac{v_t^2}{C_p^{(t)}} + \log C_p^{(t)} \right]$$

   This is the same objective as the sequential path — both minimize the Kalman filter prediction-error decomposition.

#### MLE-specific approximations and design choices

| Aspect | Choice | Rationale |
|--------|--------|----------|
| Exact 5-tuple elements | Per-timestep $(A, b, C, \eta, J)$ from Lemma 1 | Each timestep has its own Kalman gain — exact, no approximation. |
| Regularized inverse in compose | $(I + C_i J_j + \epsilon I)^{-1}$ | Guards against near-singular matrices at degenerate (NaN/zero-J) compose steps. $\epsilon = 10^{-6}$ (Float32) or $10^{-12}$ (Float64). |
| Push-through identity | $N = I - J_j M C_i$ | Derives the second inverse from the first — only one `np.linalg.inv` call per compose step. |
| Float mask blending | $A = \text{mask} \cdot A_{\text{obs}} + (1 - \text{mask}) \cdot G$ | Avoids boolean-conditioned `np.where` which can create discontinuous gradients in some AD frameworks. Arithmetic blending is smooth and AD-safe. |
| Scan output disposal | Individual `scanned.*.dispose()` after `x_filt` and `C_filt` recovery | Safe because `np.add` produces new arrays — `x_filt` and `C_filt` are independent of the scan pytree. |

### Implementation status

**The WebGPU associativeScan path is fully implemented and working** for both forward filter and backward smoother. `dlmFit` on the `webgpu` backend with `Float32` automatically uses this path (gated by `dtype === Float32 && device === 'webgpu'`). Both passes use the exact algebraic framework from Särkkä & García-Fernández [1, Lemmas 1–6].

**Forward filter** (exact 5-tuple from [1, Lemmas 1–2]):
- Constructs per-timestep 5-tuple elements $(A_t, b_t, C_t, \eta_t, J_t)$ with exact Kalman gains per Lemma 1
- First element initialized exactly from the prior $(x_0, C_0)$
- Composed via `lax.associativeScan(composeForward, elems)` (O(log n) depth) using Lemma 2 with regularized inverse and push-through identity
- Exact per-timestep Kalman gains — produces the same filtered states and covariances as the sequential filter, up to floating-point reordering
- Missing-data (NaN) support: observed timesteps use per-step Kalman gain; NaN timesteps use $A = G$, $b = 0$, $C = W$, $\eta = 0$, $J = 0$ (pure prediction)

**Backward smoother** (Särkkä & García-Fernández [1], Lemmas 5–6 — suffix scan):
- Computes exact per-timestep smoother gains $E_k = C_{filt,k} G^\top (G C_{filt,k} G^\top + W)^{-1}$ from the forward-filtered covariances via batched `np.linalg.inv`
- Constructs smoother elements $(E_k, g_k, L_k)$ where $g_k = (I - E_k G) \bar{x}_k$ and $L_k$ uses Joseph form $(I - E_k G) C_{filt,k} (I - E_k G)^\top + E_k W E_k^\top$ for guaranteed PSD
- Terminal element: $E_{n-1} = 0$, $g_{n-1} = \bar{x}_{n-1}$, $L_{n-1} = C_{filt,n-1}$
- Composed via `lax.associativeScan(compose, elems, { reverse: true })` (suffix scan, O(log n) depth)

**Math provenance summary:**

| Component | Source | Approximation |
|-----------|--------|---------------|
| Forward per-timestep elements | [1, Lemma 1] — exact Kalman gains $K_t = W F^\top S_t^{-1}$ | None — exact per-timestep gains |
| Forward prefix scan composition | [1, Lemma 2] — 5-tuple with $(I + C_i J_j + \epsilon I)^{-1}$ | Regularization $\epsilon$ for numerical stability; otherwise exact |
| Backward smoother elements | [1, Lemmas 5–6] — Theorem 2 gives smoothed density as suffix scan | None — exact per-timestep gains $E_k$ from filtered covariances |
| Backward suffix scan composition | [1, §4] — same associative law as forward | None (algebraically identical to sequential RTS) |
| Joseph form covariance | Standard Kalman filter stabilization | None — guarantees PSD by construction |

Both scans use Joseph form + symmetrization for Float32 stability. Both dispatch ⌈log₂N⌉+1 GPU kernel rounds (Kogge-Stone), giving O(log n) total depth.

Requires jax-js-nonconsuming v0.7.4 or later (project uses v0.7.7). See [upstream issue](../issues/jax-js-webgpu-jit-einsum.md) for historical context on the JIT shape inference fix for einsum inside `lax.scan` / `lax.associativeScan` on WebGPU.

### WebGPU vs WASM benchmark

`dlmFit` warm-run timings (jitted core, second of two runs):

| Model | $n$ | $m$ | wasm / f64 (scan) | webgpu / f32 (assocScan) |
|-------|-----|-----|-------------------|--------------------------|
| Nile, order=0 | 100 | 1 | <!-- timing:bb:nile-o0:wasm-f64 -->19 ms<!-- /timing --> | <!-- timing:bb:nile-o0:webgpu-f32 -->300 ms<!-- /timing --> |
| Nile, order=1 | 100 | 2 | <!-- timing:bb:nile-o1:wasm-f64 -->21 ms<!-- /timing --> | <!-- timing:bb:nile-o1:webgpu-f32 -->299 ms<!-- /timing --> |
| Kaisaniemi, trig | 117 | 4 | <!-- timing:bb:kaisaniemi:wasm-f64 -->21 ms<!-- /timing --> | <!-- timing:bb:kaisaniemi:webgpu-f32 -->339 ms<!-- /timing --> |
| Energy, trig+AR | 120 | 5 | <!-- timing:bb:trigar:wasm-f64 -->20 ms<!-- /timing --> | <!-- timing:bb:trigar:webgpu-f32 -->356 ms<!-- /timing --> |

**WebGPU scaling: O(log n) with high fixed overhead.**

A scaling benchmark (Nile order=1, m=2) measured `dlmFit` warm-run timings at exponentially increasing N (WASM: 2 warmup + 4 timed runs median; WebGPU: same). Both forward filter and backward smoother use `lax.associativeScan` on the WebGPU path:

| N | wasm/f64 | webgpu/f32 | ratio |
|---|--------------|-----------------|-------|
| 100 | <!-- timing:scale:wasm-f64:n100 -->22 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n100 -->575 ms<!-- /timing --> | 27× |
| 200 | <!-- timing:scale:wasm-f64:n200 -->22 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n200 -->631 ms<!-- /timing --> | 29× |
| 400 | <!-- timing:scale:wasm-f64:n400 -->22 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n400 -->662 ms<!-- /timing --> | 30× |
| 800 | <!-- timing:scale:wasm-f64:n800 -->23 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n800 -->677 ms<!-- /timing --> | 30× |
| 1600 | <!-- timing:scale:wasm-f64:n1600 -->23 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n1600 -->711 ms<!-- /timing --> | 31× |
| 3200 | <!-- timing:scale:wasm-f64:n3200 -->24 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n3200 -->868 ms<!-- /timing --> | 36× |
| 6400 | <!-- timing:scale:wasm-f64:n6400 -->29 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n6400 -->936 ms<!-- /timing --> | 33× |
| 12800 | <!-- timing:scale:wasm-f64:n12800 -->36 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n12800 -->974 ms<!-- /timing --> | 27× |
| 25600 | <!-- timing:scale:wasm-f64:n25600 -->53 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n25600 -->1009 ms<!-- /timing --> | 19× |
| 51200 | <!-- timing:scale:wasm-f64:n51200 -->85 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n51200 -->1076 ms<!-- /timing --> | 13× |
| 102400 | <!-- timing:scale:wasm-f64:n102400 -->160 ms<!-- /timing --> | <!-- timing:scale:webgpu-f32:n102400 -->1145 ms<!-- /timing --> | 7× |

Three findings:

1. **WASM stays flat up to N≈3200**, then grows roughly linearly (O(n)). The per-step cost asymptotes around ~1.4 µs/step at N=102400 (~<!-- timing:scale:wasm-f64:n102400 -->160 ms<!-- /timing --> total). The flat region reflects fixed JIT/dispatch overhead, not compute.

2. **WebGPU scales sub-linearly (O(log n))** — both forward and backward passes use `lax.associativeScan`, so each dispatches ⌈log₂N⌉+1 Kogge-Stone rounds. A 1024× increase from N=100 to N=102400 only doubles the runtime (<!-- timing:scale:webgpu-f32:n100 -->575 ms<!-- /timing --> → <!-- timing:scale:webgpu-f32:n102400 -->1145 ms<!-- /timing -->). However, the fixed per-dispatch overhead of WebGPU command submission is high (~500 ms base), so the constant factor dominates at practical series lengths.

3. **The WASM-to-WebGPU ratio converges as N grows**: ~27× at N=100, ~7× at N=102400. WASM is faster at all measured N, but the gap shrinks with series length because WASM's O(n) growth outpaces WebGPU's O(log n) growth. A crossover is plausible at N≈800k–1M where WASM's linear growth would overtake WebGPU's logarithmic growth.

**References:**
1. Särkkä, S. & García-Fernández, Á. F. (2020). [Temporal Parallelization of Bayesian Smoothers](https://arxiv.org/abs/1905.13002). *IEEE Transactions on Automatic Control*, 66(1), 299–306. doi:[10.1109/TAC.2020.2976316](https://doi.org/10.1109/TAC.2020.2976316). — Poses Kalman filtering and RTS smoothing as associative all-prefix-sums operations; derives the parallel scan formulation for linear-Gaussian and general Bayesian models, reducing sequential depth from O(n) to O(log n). **Lemmas 1–2**: exact parallel forward Kalman filter (5-tuple elements $(A, b, C, \eta, J)$ + associative composition with $(I+C_iJ_j)^{-1}$). **Lemmas 5–6 + Theorem 2**: parallel backward smoother. dlm-js uses Lemmas 1–2 (forward) and Lemmas 5–6 (backward).
2. Pyro contributors. [Forecasting II: state space models](https://pyro.ai/examples/forecasting_ii.html). — Demonstrates `GaussianHMM` with parallel-scan Kalman filtering on 78,888-step BART ridership data; also covers `LinearHMM` for heavy-tailed models with parallel auxiliary variable inference.
3. NumPyro contributors. [Example: Enumerate Hidden Markov Model](https://num.pyro.ai/en/latest/examples/hmm_enum.html). — Uses `scan()` with the parallel-scan algorithm of [1] to reduce parallel complexity of discrete HMM inference from O(length) to O(log(length)); demonstrates variable elimination combined with MCMC on polyphonic music data.
4. Razavi, H., García-Fernández, Á. F. & Särkkä, S. (2025). Temporal Parallelisation of Continuous-Time Maximum-a-Posteriori Trajectory Estimation. *Preprint*. — Extends the parallel associative scan framework to continuous-time MAP estimation via the Onsager–Machlup functional; yields parallel Kalman–Bucy filter and continuous-time RTS smoother as special cases. Not directly used by dlm-js (which operates in discrete time), but confirms the broader applicability of the framework.
