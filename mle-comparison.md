# Parameter estimation: MATLAB DLM vs dlm-js

> **Note:** This analysis was generated by Claude Opus 4.6 (GitHub Copilot) based on reading the MATLAB DLM source code ([`mjlaine/dlm`](https://github.com/mjlaine/dlm)) and the dlm-js `dlmMLE` implementation.

## Objective function (identical)

Both minimize the same quantity — $-2 \log L$ from the Kalman filter prediction error decomposition:

$$-2 \log L = \sum_{t=1}^{n} \left[ \frac{v_t^2}{C_p^{(t)}} + \log C_p^{(t)} \right]$$

where $v_t = y_t - F x_{t|t-1}$ is the innovation and $C_p^{(t)} = F C_{t|t-1} F' + V^2$ is the innovation covariance. The MATLAB cost function `dlm_costfun` (inside `dlmfit.m`) calls `dlmsmo(...,0,0)` (filter only, no smoother, no sample) and returns `out.lik`. The dlm-js `makeKalmanLoss` in `src/mle.ts` computes the same sum via `lax.scan` over the forward filter steps.

## Parameterization (same log-space transform)

| Aspect | MATLAB DLM | dlm-js |
|--------|-----------|--------|
| Observation noise `s` | Optionally fitted as a multiplicative factor `V·exp(θ_v)` (controlled by `options.fitv`) | Always fitted: `s = exp(θ_s)` |
| State noise `w` | `W(i,i) = exp(θ_{w,i})²` | `W(i,i) = exp(θ_{w,i})²` via `buildDiagW` |
| AR coefficients | Directly optimized (not log-transformed): `G(arinds) = θ_g` | Directly optimized (not log-transformed): `G(arInds) = θ_φ` via `buildG` rank-1 update (AD-safe) |
| Parameter grouping | `options.winds` maps W diag entries to shared parameters (e.g., `winds=[1,1,2,2]` ties states 1&2 and 3&4) | Each `W(i,i)` is an independent parameter |

Both use the same positivity enforcement: log-space for variance parameters, then `exp()` to map back. The MATLAB version has an extra feature — `winds` — that lets you **tie** W diagonal entries to shared parameters, reducing the optimization dimension when multiple states should share the same noise variance.

## Optimizer (fundamentally different)

| Aspect | MATLAB DLM | dlm-js |
|--------|-----------|--------|
| **Algorithm** | `fminsearch` (Nelder-Mead simplex) | Adam (gradient-based, 1st-order momentum) |
| **Gradient computation** | **None** — derivative-free | **Autodiff** via `valueAndGrad()` + reverse-mode AD through `lax.scan` |
| **Convergence** | Simplex shrinkage heuristic (no theoretical rate for non-convex) | Adaptive learning rate with bias-corrected moments; converges with $O(1/\sqrt{T})$ regret |
| **Per-iteration cost** | 1 forward filter pass (likelihood evaluation) | 1 forward + 1 backward pass (AD doubles the cost per iteration) |
| **Typical iterations** | 400 (`options.maxfuneval` default) | 200 (converges faster per iteration due to gradient info) |
| **Compilation** | None (interpreted MATLAB, or optional `dlmmex` C MEX) | Entire step is `jit()`-compiled: forward filter + AD + Adam update fused into one compiled kernel |
| **Jittability** | N/A | Fully jittable — optax Adam (as of v0.4.0, `count.item()` fix) |
| **WASM performance** | N/A | ~5 s for 300 iterations (Nile, n=100, m=2) |

**Key tradeoff**: Nelder-Mead needs only function evaluations (no gradients), making it trivial to implement and robust to non-smooth objectives. But it scales poorly with dimension — $O(d^2)$ simplex operations per iteration for $d$ parameters. Adam with autodiff has higher per-iteration cost (2× due to backward pass) but exploits gradient information, converging in far fewer iterations for smooth objectives like the DLM log-likelihood.

## Benchmark: same machine, same data

All timings measured on the same machine. MATLAB DLM uses Octave `fminsearch` (Nelder-Mead, `maxfuneval=400`). dlm-js uses `dlmMLE` (Adam + autodiff, `maxIter=300`, `wasm` backend). Octave timings are median of 5 runs after 1 warmup; dlm-js timings are median of 3 runs after 1 warmup.

| Model | n | m | params | Octave `fminsearch` | dlm-js `dlmMLE` (wasm) | -2logL (Octave) | -2logL (dlm-js) |
|-------|---|---|--------|---------------------|------------------------|-----------------|-----------------|
| Nile, order=1, fit s+w | 100 | 2 | 3 | 2827 ms | 3707 ms | 1104.6 | 1105.0 |
| Nile, order=1, fit w only | 100 | 2 | 2 | 1623 ms | — | 1104.7 | — |
| Nile, order=0, fit s+w | 100 | 1 | 2 | 610 ms | 1412 ms | 1095.8 | 1095.9 |
| Kaisaniemi, trig, fit w | 117 | 4 | 4 | **failed** (NaN/Inf) | 6230 ms | — | 341.6 |
| Energy, trig+AR, fit s+w+φ | 120 | 5 | 7 | — | 7100 ms | — | 443.8 |

**Key observations:**
- On the Nile data, Octave `fminsearch` is faster despite being an interpreted language — the Kalman filter is just matrix multiplications in a loop, where Octave's LAPACK-backed vectorized ops are efficient. dlm-js pays for JIT compilation overhead that doesn't amortize on a 100-observation dataset.
- Both converge to essentially the same -2logL (within 0.4), confirming equivalent objective functions.
- On the Kaisaniemi model (m=4, 4 parameters), **Octave's Nelder-Mead fails entirely** — the simplex diverges and produces NaN/Inf. dlm-js converges in 300 iterations (6.3 s) thanks to gradient information guiding the optimizer in the 4-dimensional parameter space. This demonstrates the practical advantage of gradient-based optimization for higher-dimensional models.
- dlm-js `dlmMLE` always fits both `s` and `w` jointly, while MATLAB DLM can fit `w` only (`fitv=0`), which is faster when `s` is known.

## MCMC (MATLAB-only feature)

The MATLAB DLM has a second estimation mode (`options.mcmc=1`) that uses Adaptive Metropolis (AM) MCMC via Marko Laine's `mcmcrun` toolbox:

- Runs 5000 simulations (default `options.nsimu`)
- Normal prior on log-transformed parameters with configurable CVs (`options.varcv`, `options.vcv`)
- Returns full posterior chain, credible intervals, and point estimate (posterior mean of last half)
- Uses disturbance smoother for Gibbs-style state sampling

**dlm-js has no MCMC equivalent.** `dlmMLE` returns a point estimate only. Adding uncertainty quantification would require either:
- Porting `mcmcrun` (significant effort — it's a separate toolbox)
- Using the Hessian at the MLE optimum for approximate confidence intervals (cheap — just wrap `valueAndGrad` in another `grad`)
- Implementing stochastic gradient MCMC (e.g., SGLD) using the existing AD infrastructure

## Feature comparison summary

| Capability | MATLAB DLM | dlm-js `dlmMLE` |
|-----------|-----------|-----------------|
| MLE point estimate | ✅ `fminsearch` | ✅ Adam + autodiff |
| Gradient-based optimization | ❌ | ✅ |
| JIT compilation of optimizer | ❌ | ✅ |
| Fit observation noise `s` | ✅ (optional via `fitv`) | ✅ (always) |
| Fit state noise `w` | ✅ | ✅ |
| Fit AR coefficients `arphi` | ✅ | ✅ (`fitar: true`) |
| Tie W parameters (`winds`) | ✅ | ❌ (each W entry independent) |
| Custom cost function | ✅ (`options.fitfun`) | ❌ |
| MCMC posterior sampling | ✅ (Adaptive Metropolis via `mcmcrun`) | ❌ |
| State sampling for Gibbs | ✅ (disturbance smoother) | ❌ |
| Posterior uncertainty | ✅ (full chain) | ❌ (point estimate only) |
| Convergence diagnostic | ✅ (`chain`, `sschain`) | ✅ (`likHistory`) |
| Runs in browser | ❌ | ✅ |
| MEX/WASM acceleration | ✅ (`dlmmex` optional) | ✅ (`wasm` backend; see [benchmark](#benchmark-same-machine-same-data)) |

## What dlm-js does differently (and arguably better)

1. **Exact gradients** vs finite-difference-free simplex — Adam converges more reliably on smooth likelihoods, especially in higher dimensions where Nelder-Mead may fail entirely (see Kaisaniemi benchmark above).
2. **Full JIT compilation** — the entire optimization step (forward filter + AD + parameter update) compiles to a single fused kernel. JIT overhead currently dominates for small datasets (n=100); the advantage grows with larger n or more complex models.
3. **WASM backend** — runs in Node.js and the browser without native dependencies.
4. **Robust in higher dimensions** — gradient-based optimization handles 4+ parameters where Nelder-Mead diverges.
5. **Joint AR coefficient estimation** — `fitar: true` jointly estimates observation noise, state variances, and AR coefficients in a single autodiff pass. The AR coefficients enter the G matrix via AD-safe rank-1 updates (`buildG`), keeping the entire optimization `jit()`-compilable.

## What MATLAB DLM does that dlm-js doesn't (yet)

1. **MCMC posterior sampling** — full Bayesian uncertainty quantification with priors.
2. **Parameter tying** (`winds`) — reduces optimization dimension for structured models.
3. **Custom fit functions** (`options.fitfun`) — user-supplied cost functions.
4. **V factor fitting** (`options.fitv`) — fits a multiplicative factor on V rather than V directly (useful when V is partially known from instrument specification).
